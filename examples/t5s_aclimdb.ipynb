{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5s_aclimdb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjXNZIm/p/47srKld4YCeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honzas83/t5s/blob/main/examples/t5s_aclimdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZYzutIFUEN"
      },
      "source": [
        "# Sentiment analysis using the t5s library\n",
        "## Install the t5s library and its dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adZxzMjPgaF0"
      },
      "source": [
        "%%capture pip_install\n",
        "!pip install git+https://github.com/honzas83/t5s --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq65hhjaFZHJ"
      },
      "source": [
        "## Download and extract the ACL IMDB corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA61SgeTgiyK"
      },
      "source": [
        "!curl http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz | tar xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0HajwkYFfWT"
      },
      "source": [
        "## Download the T5 SentencePiece model\n",
        "\n",
        "This is the standard SentecePiece model provided by Google for their pre-trained T5 model. The `t5-base` model is downloaded by the `t5s` library (via the Huggingface Transformers library). The `gsutil` command copies the file from Google Cloud Storage bucket to the local directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_H1sepNjmLm"
      },
      "source": [
        "!gsutil cp -r gs://t5-data/vocabs/cc_all.32000/ ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHMB0aHMhR3u"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFVGBWX1k3Mj"
      },
      "source": [
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "logging.basicConfig()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRoV8qmPF_hn"
      },
      "source": [
        "## Convert the dataset formats\n",
        "\n",
        "The ACL IMDB dataset consists of a set of TXT files in `pos` and `neg` directories. We use `glob` to search such files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTcDMDy1g-Bf"
      },
      "source": [
        "def find_data(dn):\n",
        "    fns = glob(os.path.join(dn, \"pos\", \"*.txt\"))+glob(os.path.join(dn, \"neg\", \"*.txt\"))\n",
        "    return fns\n",
        "\n",
        "def convert_data(fns, out_fn):\n",
        "    with open(out_fn, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for fn in fns:\n",
        "            if \"/pos/\" in fn:\n",
        "                label = \"positive\"\n",
        "            elif \"/neg/\" in fn:\n",
        "                label = \"negative\"\n",
        "            else:\n",
        "                continue\n",
        "            with open(fn, \"r\", encoding=\"utf-8\") as fr:\n",
        "                text = fr.read().strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            \n",
        "            text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "            print(text, label, sep=\"\\t\", file=fw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcaxU3UxGadB"
      },
      "source": [
        "We search for all `*.txt` files in the train subdirectory, then we suffle the filenames and we leave 2k files as the development set. The rest is used as the train data.\n",
        "\n",
        "The `*.txt` files converted to tab-separated values (TSV) format using the `convert_data()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tksD1rrnhApj"
      },
      "source": [
        "train_fns = find_data(\"aclImdb/train\")\n",
        "random.shuffle(train_fns)\n",
        "dev_fns = train_fns[-2000:]\n",
        "del train_fns[-2000:]\n",
        "convert_data(train_fns, \"aclImdb.train.tsv\")\n",
        "convert_data(dev_fns, \"aclImdb.dev.tsv\")\n",
        "test_fns = find_data(\"aclImdb/test\")\n",
        "convert_data(test_fns, \"aclImdb.test.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxqF5F1uG8pW"
      },
      "source": [
        "## t5s configuration\n",
        "\n",
        "The configuration consists of different sections:\n",
        "\n",
        "### `tokenizer`\n",
        "\n",
        "*   `spm` - the name of the SentencePiece model\n",
        "\n",
        "### `t5_model`\n",
        "\n",
        "* `pre_trained` - the name of the pre-trained model to load for fine-tuning,\n",
        "* `save_checkpoint` - save fine-tuned checkpoints under this name,\n",
        "* `save_checkpoint_every` - integer, which specifies how often the checkpoints are saved, e.g. the value 1 means save every epoch.\n",
        "\n",
        "### `dataset`\n",
        "\n",
        "* `*_tsv` - names of TSV files used as training, development and test sets,\n",
        "* `loader` - specification how to load the training data\n",
        "  * `loader.input_size` - maximum number of input tokens in the batch\n",
        "  * `loader.output_size` - maximum number of output tokens in the batch\n",
        "  * `loader.min_batch_size` - minimum number of examples in the batch. Together with `input_size` and `output_size` specifies the maximum length of an input and an output sequence (`input_size//min_batch_size`, `output_size//min_batch_size`).\n",
        "\n",
        "### `training`\n",
        "\n",
        "* `shared_trainable` - boolean, if `True`, the parameters of shared embedding layer are trained,\n",
        "* `encoder_trainable` - boolean, if `True`, the parameters of the encoder are trained,\n",
        "* `n_epochs` - number of training epochs,\n",
        "* `initial_epoch` - number of training epochs already performed, the next epoch will be `initial_epoch+1`,\n",
        "* `steps_per_epoch` - the length of each epoch in steps, if ommited, the epoch means one pass over the training TSV,\n",
        "* `learning_rate` - initial learning rate for `epoch=1`\n",
        "* `learning_rate_schedule` - boolean, if `True`, the sqrt learning rate schedule is used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImjrvObYi5Hd"
      },
      "source": [
        "config = {\n",
        "    \"tokenizer\": {\n",
        "        \"spm\": \"cc_all.32000/sentencepiece.model\",\n",
        "    },\n",
        "    \"t5_model\": {\n",
        "        \"pre_trained\": \"t5-base\",\n",
        "        \"save_checkpoint\": \"T5_aclImdb\",\n",
        "        \"save_checkpoint_every\": 1,\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"train_tsv\": \"aclImdb.train.tsv\",\n",
        "        \"devel_tsv\": \"aclImdb.dev.tsv\",\n",
        "        \"test_tsv\": \"aclImdb.test.tsv\",\n",
        "        \"loader\": {\n",
        "            \"input_size\": 3072,\n",
        "            \"output_size\": 256,\n",
        "            \"min_batch_size\": 4,\n",
        "        },\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"shared_trainable\": False,\n",
        "        \"encoder_trainable\": True,\n",
        "        \"n_epochs\": 1,\n",
        "        \"initial_epoch\": 0,\n",
        "        \"steps_per_epoch\": 500,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"learning_rate_schedule\": True,\n",
        "    },\n",
        "    \"predict\": {\n",
        "        \"batch_size\": 50,\n",
        "        \"max_input_length\": 768,\n",
        "        \"max_output_length\": 64,\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OizcHk5JYIx"
      },
      "source": [
        "### Import the t5s library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ampv7fC5krW-"
      },
      "source": [
        "from t5s import T5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh9Kjk2IJbeW"
      },
      "source": [
        "### Instantiate the T5 class and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mre0IQ5AktUF"
      },
      "source": [
        "t5 = T5(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBvYw2fmkwJn"
      },
      "source": [
        "t5.fine_tune()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI93aR7AyMG-"
      },
      "source": [
        "## Predict using the model\n",
        "\n",
        "The use the T5 model in code, use `predict()` method. To evaluate the model, the `predict_tsv()` could be more useful, together with evaluation using the `eval_tsv.py` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qjVbRHlyStn"
      },
      "source": [
        "batch = []\n",
        "reference = []\n",
        "with open(\"aclImdb.dev.tsv\", \"r\") as fr:\n",
        "    for line in fr:\n",
        "        line = line.strip()\n",
        "        batch.append(line.split(\"\\t\")[0])\n",
        "        reference.append(line.split(\"\\t\")[1])\n",
        "        if len(batch) >= 10:\n",
        "            break\n",
        "print(reference)\n",
        "print(t5.predict(batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoUurNWKpANN"
      },
      "source": [
        "t5.predict_tsv(\"aclImdb.dev.tsv\", \"aclImdb.dev.pred.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrzVASkkyJ-h"
      },
      "source": [
        "The evaluation script `eval_tsv.py` takes 3 parameters - the name of metrics to compute, reference TSV and predicted TSV. The `match` metric computes sentence accuracy `SAcc` and word-level accuracy `WAcc`. The output also contains the number of correct and erroneous sentences and words. The output is in the JSON format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBqIJqVTxCf0"
      },
      "source": [
        "!eval_tsv.py match aclImdb.dev.tsv aclImdb.dev.pred.tsv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}