{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5s_aclimdb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0fUjdOI93jFves9kq10Hv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honzas83/t5s/blob/main/examples/t5s_aclimdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZYzutIFUEN"
      },
      "source": [
        "# Sentiment analysis using the t5s library\n",
        "## Install the t5s library and its dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adZxzMjPgaF0",
        "outputId": "22390146-08da-4434-aa01-e7b96bd3ff17"
      },
      "source": [
        "%% capture pip_install\n",
        "!pip install git+https://github.com/honzas83/t5s --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/honzas83/t5s\n",
            "  Cloning https://github.com/honzas83/t5s to /tmp/pip-req-build-2y_582bu\n",
            "  Running command git clone -q https://github.com/honzas83/t5s /tmp/pip-req-build-2y_582bu\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5s==0.1) (0.1.94)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow in /usr/local/lib/python3.6/dist-packages (from t5s==0.1) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-text in /usr/local/lib/python3.6/dist-packages (from t5s==0.1) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: transformers==3.3.0 in /usr/local/lib/python3.6/dist-packages (from t5s==0.1) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from t5s==0.1) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5s==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (1.33.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->t5s==0.1) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (0.8.1rc2)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0->t5s==0.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5s==0.1) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.0->t5s==0.1) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.0->t5s==0.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0->t5s==0.1) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0->t5s==0.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0->t5s==0.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0->t5s==0.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->t5s==0.1) (3.1.0)\n",
            "Building wheels for collected packages: t5s\n",
            "  Building wheel for t5s (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for t5s: filename=t5s-0.1-cp36-none-any.whl size=13589 sha256=f79c1523ef24169e00a90d6be229191bfcd747b0570622c6fa534c68eb717dd1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6gaks6we/wheels/04/e5/71/24b59a9d225bfaead43ca97afe95fce46b5d56ddba98ac4b2d\n",
            "Successfully built t5s\n",
            "Installing collected packages: t5s\n",
            "  Found existing installation: t5s 0.1\n",
            "    Uninstalling t5s-0.1:\n",
            "      Successfully uninstalled t5s-0.1\n",
            "Successfully installed t5s-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq65hhjaFZHJ"
      },
      "source": [
        "## Download and extract the ACL IMDB corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA61SgeTgiyK",
        "outputId": "aae815e8-82c2-478a-992e-de63986f5ca5"
      },
      "source": [
        "!curl http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz | tar xz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-10 14:26:28--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.4’\n",
            "\n",
            "aclImdb_v1.tar.gz.4 100%[===================>]  80.23M  23.3MB/s    in 3.4s    \n",
            "\n",
            "2020-12-10 14:26:31 (23.3 MB/s) - ‘aclImdb_v1.tar.gz.4’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0HajwkYFfWT"
      },
      "source": [
        "## Download the T5 SentencePiece model\n",
        "\n",
        "This is the standard SentecePiece model provided by Google for their pre-trained T5 model. The `t5-base` model is downloaded by the `t5s` library (via the Huggingface Transformers library). The `gsutil` command copies the file from Google Cloud Storage bucket to the local directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_H1sepNjmLm",
        "outputId": "ef9deb1c-785d-4062-9ed9-3cfad276add9"
      },
      "source": [
        "!gsutil cp -r gs://t5-data/vocabs/cc_all.32000/ ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://t5-data/vocabs/cc_all.32000/sentencepiece.model...\n",
            "Copying gs://t5-data/vocabs/cc_all.32000/sentencepiece.vocab...\n",
            "/ [2 files][  1.3 MiB/  1.3 MiB]                                                \n",
            "Operation completed over 2 objects/1.3 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHMB0aHMhR3u"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRoV8qmPF_hn"
      },
      "source": [
        "## Convert the dataset formats\n",
        "\n",
        "The ACL IMDB dataset consists of a set of TXT files in `pos` and `neg` directories. We use `glob` to search such files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTcDMDy1g-Bf"
      },
      "source": [
        "def find_data(dn):\n",
        "    fns = glob(os.path.join(dn, \"pos\", \"*.txt\"))+glob(os.path.join(dn, \"neg\", \"*.txt\"))\n",
        "    return fns\n",
        "\n",
        "def convert_data(fns, out_fn):\n",
        "    with open(out_fn, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for fn in fns:\n",
        "            if \"/pos/\" in fn:\n",
        "                label = \"positive\"\n",
        "            elif \"/neg/\" in fn:\n",
        "                label = \"negative\"\n",
        "            else:\n",
        "                continue\n",
        "            with open(fn, \"r\", encoding=\"utf-8\") as fr:\n",
        "                text = fr.read().strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            \n",
        "            text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "            print(text, label, sep=\"\\t\", file=fw)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcaxU3UxGadB"
      },
      "source": [
        "We search for all `*.txt` files in the train subdirectory, then we suffle the filenames and we leave 2k files as the development set. The rest is used as the train data.\n",
        "\n",
        "The `*.txt` files converted to tab-separated values (TSV) format using the `convert_data()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tksD1rrnhApj"
      },
      "source": [
        "train_fns = find_data(\"aclImdb/train\")\n",
        "random.shuffle(train_fns)\n",
        "dev_fns = train_fns[-2000:]\n",
        "del train_fns[-2000:]\n",
        "convert_data(train_fns, \"aclImdb.train.tsv\")\n",
        "convert_data(dev_fns, \"aclImdb.dev.tsv\")\n",
        "test_fns = find_data(\"aclImdb/test\")\n",
        "convert_data(test_fns, \"aclImdb.test.tsv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxqF5F1uG8pW"
      },
      "source": [
        "## t5s configuration\n",
        "\n",
        "The configuration consists of different sections:\n",
        "\n",
        "### `tokenizer`\n",
        "\n",
        "*   `spm` - the name of the SentencePiece model\n",
        "\n",
        "### `t5_model`\n",
        "\n",
        "* `pre_trained` - the name of the pre-trained model to load for fine-tuning,\n",
        "* `save_checkpoint` - save fine-tuned checkpoints under this name,\n",
        "* `save_checkpoint_every` - integer, which specifies how often the checkpoints are saved, e.g. the value 1 means save every epoch.\n",
        "\n",
        "### `dataset`\n",
        "\n",
        "* `*_tsv` - names of TSV files used as training, development and test sets,\n",
        "* `loader` - specification how to load the training data\n",
        "  * `loader.input_size` - maximum number of input tokens in the batch\n",
        "  * `loader.output_size` - maximum number of output tokens in the batch\n",
        "  * `loader.min_batch_size` - minimum number of examples in the batch. Together with `input_size` and `output_size` specifies the maximum length of an input and an output sequence (`input_size//min_batch_size`, `output_size//min_batch_size`).\n",
        "\n",
        "### `training`\n",
        "\n",
        "* `shared_trainable` - boolean, if `True`, the parameters of shared embedding layer are trained,\n",
        "* `encoder_trainable` - boolean, if `True`, the parameters of the encoder are trained,\n",
        "* `n_epochs` - number of training epochs,\n",
        "* `initial_epoch` - number of training epochs already performed, the next epoch will be `initial_epoch+1`,\n",
        "* `steps_per_epoch` - the length of each epoch in steps, if ommited, the epoch means one pass over the training TSV,\n",
        "* `learning_rate` - initial learning rate for `epoch=1`\n",
        "* `learning_rate_schedule` - boolean, if `True`, the sqrt learning rate schedule is used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImjrvObYi5Hd"
      },
      "source": [
        "config = {\n",
        "    \"tokenizer\": {\n",
        "        \"spm\": \"cc_all.32000/sentencepiece.model\",\n",
        "    },\n",
        "    \"t5_model\": {\n",
        "        \"pre_trained\": \"t5-base\",\n",
        "        \"save_checkpoint\": \"T5_aclImdb\",\n",
        "        \"save_checkpoint_every\": 1,\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"train_tsv\": \"aclImdb.train.tsv\",\n",
        "        \"devel_tsv\": \"aclImdb.dev.tsv\",\n",
        "        \"test_tsv\": \"aclImdb.test.tsv\",\n",
        "        \"loader\": {\n",
        "            \"input_size\": 3072,\n",
        "            \"output_size\": 256,\n",
        "            \"min_batch_size\": 4,\n",
        "        },\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"shared_trainable\": False,\n",
        "        \"encoder_trainable\": True,\n",
        "        \"n_epochs\": 20,\n",
        "        \"initial_epoch\": 0,\n",
        "        \"steps_per_epoch\": 1000,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"learning_rate_schedule\": True,\n",
        "    },\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OizcHk5JYIx"
      },
      "source": [
        "### Import the t5s library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ampv7fC5krW-"
      },
      "source": [
        "from t5s import T5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh9Kjk2IJbeW"
      },
      "source": [
        "### Instantiate the T5 class and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mre0IQ5AktUF"
      },
      "source": [
        "t5 = T5(config)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBvYw2fmkwJn",
        "outputId": "1c877bfc-a180-4567-9414-2a0466ec8cd5"
      },
      "source": [
        "t5.fine_tune()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing T5Training.\n",
            "\n",
            "Some weights of T5Training were not initialized from the model checkpoint at t5-base and are newly initialized: ['loss']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 1/20\n",
            "1000/1000 [==============================] - ETA: 0s - sent_accuracy: 0.8541 - edit_accuracy: 0.9267 - loss: 0.2079\n",
            "1000/1000 [==============================] - 942s 942ms/step - sent_accuracy: 0.8541 - edit_accuracy: 0.9267 - loss: 0.2079 - val_sent_accuracy: 0.9050 - val_edit_accuracy: 0.9525 - val_loss: 0.1284\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007071067811865475.\n",
            "Epoch 2/20\n",
            " 834/1000 [========================>.....] - ETA: 2:23 - sent_accuracy: 0.8888 - edit_accuracy: 0.9444 - loss: 0.1496"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}